% JuliaCon proceedings template
\documentclass{juliacon}
\setcounter{page}{1}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{amsmath}
%\bibliographystyle{juliacon}

\usepackage[labelformat=simple]{subfig}

\renewcommand\thesubfigure{\arabic{subfigure}}
\renewcommand{\subfigurename}{Figure}


\begin{document}

\input{header}

\maketitle

\begin{abstract}

Probability bounds analysis combines interval arithmetic with probability theory, and provides a representation of sets of distributions in structures called probability boxes (p-boxes), which generalise both distribution functions and intervals. P-boxes generally return interval bounds on all probabilistic quantities, for example samples, cdfs, and probability measures are all intervals. This framework also allows for the comprehensive propagation of probabilities through calculations in a rigorous way, in a similar fashion that interval arithmetic does for sets. As such, ProbabilityBoundsAnalysis.jl gives a rigorous arithmetic of random variables, where both marginal (univariate) and dependency information can be known, partially known or missing completely. We describe the main theoretical elements of probability bounds analysis, and describe a minimal implementation of the method and it's main algorithms in Julia.

\end{abstract}

\section{Introduction}
\label{sec:intro}


An arithmetic of probability distributions has held a long interest among mathematicians and scientists:
\begin{displayquote}
  A question asked of Kolmogorov, \\
  answered for the sum by Marakov, \\
  partially answered by Sklar and Frank (for which the copula was invented),\\
  made algorithmically available by Williamson, \\
  and generalised by others.
\end{displayquote}

\noindent Indeed it was Kolmogorov who originally asked about the result of a sum of two distributions without knowing their joint distribution. This was answered for the sum by Marakov \cite{makarov1982estimates}, who showed the result was a set of distributions and was able to provide bounds on this function. Sklar, Schweizer and Frank generalised this result to other (positive) binary operations \cite{frank1987best,schweizer2011probabilistic}. In this pursuit they created copulas, a general way to encode probabilistic dependence independently from marginals, and a now essential object used in probabilistic modelling. In his seminal dissertation \cite{williamson1989probabilistic}, Williamson described an algorithm for efficiently performing these arithmetic operations, which can give guaranteed bounds on probability distributions in terms of an upper and lower cdf. He called his method \textit{Probabilistic Arithmetic} and his sets of distributions \textit{Dependency Bounds}. Since then the method has been generalised \cite{ferson2015constructing,ferson1996whereof,ferson2004arithmetic} to most of the base binary and unary operations that would be present in a programming language. Probability boxes (p-boxes) are the name now given to these structures, and Probability Bounds Analysis (PBA) the name of the method.

The goal of PBA can be stated as \textbf{to compute guaranteed bounds on functions of random variables given only partial knowledge of the input probability distributions and their dependencies}. That is to compute with partial knowledge about the input joint distribution. Ideally all of the available information about random variables should be used, but no more than what actually is available.

The idea of bounding probability has a very long tradition throughout the history of probability theory. George Boole \cite{boole1854investigation, hailperin1986boole} used the notion of interval bounds on probability. Chebyshev \cite{chebyshev1874valeurs} described bounds on a distribution when only the mean and variance of the variable are known, and Markov \cite{markoff1900question} found bounds on a positive variable when only the mean is known.  Fréchet \cite{frechet1935generalisation} discovered how to bound joint distributions solely from knowing the marginal distributions, without making independence assumptions. Bounding probabilities has continued to the present day, culminating into the modern theory of Imprecise Probabilities \cite{walley1991statistical, klir2013uncertainty, troffaes2014lower, augustin2014introduction}.

Imprecise probability is effectively a generalisation of probability theory where uncertainty can be expressed about the probability measure. This is particularly relevant when information is scarce, unreliable, vague, conflicting or imprecise. In such cases defining a unique probability distribution is difficult. P-boxes are one of many ways to describe a set of distributions, others include: Dempster-Shafer structures \cite{dempster2008upper,shafer1976mathematical}, random sets \cite{molchanov2005theory}, possibility distributions \cite{zadeh1978fuzzy,dubois1988possibility, hose2019possibilistic} and lower previsions \cite{troffaes2014lower}. These structures were discovered independently, but are often synonymous and can be translated from one to another, with different degrees of generality. Imprecise probability links all these theories into one. For a comprehensive overview of the theory, and a for a formal description of uncertainty and information in terms of these structures, \cite{klir2013uncertainty} is recommended. In that sense PBA is a part of imprecise probabilities but provides a framework for computing with p-boxes.



\iffalse
Probability bounds analysis is a combination of the methods of standard interval analysis \cite{moore1996interval, jaulin2001interval} and classical probability theory (see, inter alia, \cite{feller1968probability, feller1971probability}).  The idea of bounding probability has a very long tradition throughout the history of probability theory.  Indeed, George Boole \cite{boole1854investigation, hailperin1986boole} used the notion of interval bounds on probability.  Chebyshev \cite{chebyshev1874valeurs} described bounds on a distribution when only the mean and variance of the variable are known, and Markov \cite{markoff1900question} found bounds on a positive variable when only the mean is known.  Fréchet \cite{frechet1935generalisation} discovered how to make calculations with uncertain estimates of joint probabilities without making independence assumptions.  Bounding probabilities has continued to the present day \cite{walley1991statistical, klir2013uncertainty, troffaes2014lower, augustin2014introduction}, culminating into the modern theory of Imprecise Probabilities.

An arithmetic of probability distributions has held a long interest among mathematicians and engineers




Computations with probabilities are usually performed by Monte-Carlo style simulations, where essentially many random realisations of functions are required to be run. These sampling methods require many thousands of realisations to be accurate, and even then will only produce an approximation of the desired probabilistic quantity. In contrast, the methods ProbabilityBoundsAnalysis.jl are exact rather than approximate, and give no restriction to the distribution shape or dependency.
\fi



\begin{figure*}[htp]

  \centering
  \includegraphics[width=.3\textwidth]{../examples/JuliaCon/fig1/fig1_dist.pdf}\hfill
  \includegraphics[width=.3\textwidth]{../examples/JuliaCon/fig1/fig1_interval.pdf}\hfill
  \includegraphics[width=.3\textwidth]{../examples/JuliaCon/fig1/fig1_pbox.pdf}
  
  \caption{A precise distribution, an interval and a probability box}
  \label{fig:figure1}
  
\end{figure*}

\section{Probability Boxes}
\label{sec:pboxes}

A real-valued random variable is characterised by its distribution function $F$, which is a monotonically increasing function from the real numbers onto the interval $[0, 1]$ such that the value of the function at negative infinity is zero and the value of the function at positive infinity is one.  A p-box consists of a pair of such functions that are used to circumscribe an imprecisely known distribution function $F$. The p-box, consisting of the pair of bounding functions, identifies a set of probability distributions that lie entirely within these bounds. Additional information about the random variable may be available, such as bounds on its mean and variance and its distribution family, which may be used to further restrict the set of distributions. A p-box is thus defined by the following three constrains: (1) two bounding cumulative distribution functions (cdf), (2) interval bounds on the mean and variance, and (3) a collection of distribution families:

\begin{enumerate}
  \item $\begin{aligned}[t]
    \underline{F}(x) \leq F(x) \leq \overline{F}(x) \\%&&&& \text{(Bounds on the cdf)} \\
  \end{aligned}$
  \item $\begin{aligned}[t]
    \mu &\in [\underline{ \mu }, \overline{ \mu }]  \\%&&&& \text{(bounds on mean and variance)}\\
    \sigma^2 &\in [\underline{\sigma}^2 , \overline{\sigma}^2]
  \end{aligned}$
  \item $\begin{aligned}[t]
      F \in \bold{F} \\%&&&&&&& \text{(distribution family)} \\
  \end{aligned}$
  \end{enumerate}

\noindent That is, a random variable is a member of a p-box if its cdf $F$ falls within the cdf bounds of the p-box $F(x) \in [\underline{F}(x), \overline{F}(x)]$ for all $x$, its moments are inside the interval moments of the p-box, and it belongs to a family of distribution functions (e.g. normal, uniform) considered by the p-box. Some of the constraints may be missing. For example, if the distribution family is unknown then the set is defined solely from the cdf and moment bounds. Such p-boxes are sometimes called non-parametric, since its members do not belong to any particular class of distribution. Some constraints may also be inferred from others. For example, the interval moments may be bounded from the cdf bounds, and cdf may be bounded from moment information (explored further in section \ref{section:Moments}). Figure \ref{fig:figure1} shows an example of a distribution function, interval and a p-box. The grey shaded region in the interval and p-box bounds a (potentially infinite) collection of probability distributions.

All of a p-box's probabilistic quantities are intervals. The cdf of a p-box may be found by: 

\begin{equation*}
  [\underline{F}(x), \overline{F}(x)] ,
\end{equation*}

\noindent a sample of the p-box may be drawn using the inverse cdfs: 

\begin{equation*}
  [\underline{F}^{-1}(\alpha), \overline{F}^{-1}(\alpha)]
\end{equation*}

\noindent where $\alpha \sim U(0,1)$ is a sample from a uniform distribution, and the probability measure\footnote{The probability measure is a function which returns the probability that the random variable is in some set} on some interval $U = [a, b]$ is bounded as follows:

\begin{align*}
  \underline{\mathbb{P}}(U) &= \text{max}(0, \underline{F}(b) - \overline{F}(a)) \\ 
  \overline{\mathbb{P}}(U)  &= \overline{F}(b) - \underline{F}(a) ,
\end{align*}

where the max operator is required when $\underline{F}(b) < \overline{F}(a)$. Note the same can be achieved by using the standard formula for finding the measure from the cdf, $\mathbb{P}(U) = F(b)- F(a)$, and using interval arithmetic.


P-boxes generalise precise distributions and intervals in the following way. A distribution is a p-box with a precise cdf and moments, i.e. when: 

\begin{align*}
  \underline{F}(x) &= \overline{F}(x), \\ 
  \underline{\mu}  &= \overline{\mu}, \\ 
  \underline{\sigma}^2 &= \overline{\sigma}^2
\end{align*}

\noindent and an interval $[a,b]$ is a p-box whose bounds are step functions: 

\begin{align*}
    \underline{F}(x) &= \epsilon_{b}(x)\\
    \overline{F}(x) &= \epsilon_{a}(x) ,
\end{align*}

\noindent where $\epsilon_k$ is: 

\begin{equation*}
   \epsilon_k(x) = \begin{cases} 0 &\text{when } x < k \\ 1 &\text{when } x \geq k \end{cases}
\end{equation*}


\begin{figure*}[htp]

  \centering
  \includegraphics[width=.3\textwidth]{../examples/JuliaCon/fig2/fig2_pbox2.pdf}\hfill
  \includegraphics[width=.3\textwidth]{../examples/JuliaCon/fig2/fig2_pbox1.pdf}\hfill
  \includegraphics[width=.3\textwidth]{../examples/JuliaCon/fig2/fig2_pbox3.pdf}
  
  \caption{Various outer representations of distribution functions, left $4$ steps and centre $25$ steps, and a p-box (right) with 10 steps}
  \label{fig:figure2}
  
\end{figure*}


Moreover, theoretical bounds on the mean and variance of an interval can be found [CITE SCOTT]:

\begin{align*}
  \mu &\in [a, b] \\
  \sigma^2 &\in [0, (b - a)^{2/4}]
\end{align*}

That is, it is not possible to find a distribution whose range is in $[a, b]$ and whose variance is greater than $(b-a)^{2/4}$. The lower bound on the variance is zero, since scalars are also included in the interval. Under this definition of an interval, a random sample will always be the interval $[a, b]$, the cdf returns 0 when $x < a$, the interval $[0,1]$ when $a <= x < b$, and $1$ when $b <= x$. Further, the probability measure of the interval $X = [a, b]$ will be: 

\begin{equation*}
  \mathbb{P}_{X}(U) = \begin{cases}
    0 & \text{when } U \cap [a,b] = \emptyset \\
    1 & \text{when } [a,b] \subseteq U  \\
    [0, 1] & \text{otherwise }\\
  \end{cases}
\end{equation*}

\noindent That is, if the set $U$ does not intersect the interval $X$, $\mathbb{P}_{X}=0$, i.e. $U$ certainly does not contain any of the random variables. If $X$ is fully contained in $U$, $\mathbb{P}_{X}=1$, i.e. $U$ certainly contains all of the random variables. And finally if $U$ intersects (but does not contain) $X$, the probability measure is the vacuous probability interval $\mathbb{P}_{X} = [0, 1]$, i.e. we are completely uncertain about the containment. Note that the $P_{X} = \{0, 1, [0,1]\}$ is due to the interval bounds being step functions. Generally p-boxes may yield other probability intervals.

\subsection{Outer representations of p-boxes}
\label{sec:outer_approx}

An important feature of probability bounds analysis is how distribution functions and p-boxes are represented. Generally, analytical solutions for cdfs are not readily available, for example the normal distribution's cdf can only be found by integrating the probability density, usually with quadrature. However even if the functions were available analytically, when the variables are used in an arithmetic operation the output distribution will not necessarily belong to the same family. Therefore we require a representation of these continuous functions which is not dependent on the distribution family (for example as used in polynomial chaos expansion), and ideally is robust in the sense that it can produce an interval error for the numerical representation. Such a representation of a probability is sometimes called an outer approximation, in the same fashion an interval is an outer approximation of a floating point number, and which contracts to the exact value as more computational resources is used.

We follow the representation first introduced by Williamson and Downs \cite{williamson1990probabilistic}, where a discrete upper and lower approximations of distribution functions are constructed using the inverse cdfs. Note that when considering inverse cdfs $\underline{F}^{-1}(p) \geq F^{-1}(p) \geq \overline{F}^{-1}(p)$, for $p \in [0,1]$. An outer approximation using two finite vectors\footnote{We begin vector indexing with $1$ as is done in the Julia language.} $u$ and $d$ of length $N$ is constructed by evaluating the inverses for uniformly spaced probability levels $p_{i} = \frac{i-1}{N}$ for $i = 1, ... , N+1$, e.g. $p_{i} = [0, 0.25, 0.5, 0.75, 1]$ for $N = 4$. The vectors $u$ and $d$ are defined as follows:

\begin{align*}
  u[i] &= \overline{F}^{-1}(p_{i}) \\ 
  d[i] &= \underline{F}^{-1}(p_{i+1}) & \text{for } i = 1, ..., N
\end{align*}



\begin{figure*}[htp]

  \centering
  \includegraphics[width=.25\textwidth]{../examples/JuliaCon/fig3/normal.pdf}\hfill
  \includegraphics[width=.25\textwidth]{../examples/JuliaCon/fig3/uniform.pdf}\hfill
  \includegraphics[width=.25\textwidth]{../examples/JuliaCon/fig3/beta.pdf}\hfill
  \includegraphics[width=.25\textwidth]{../examples/JuliaCon/fig3/gamma.pdf}
  

  \includegraphics[width=.25\textwidth]{../examples/JuliaCon/fig3/lognormal.pdf}\hfill
  \includegraphics[width=.25\textwidth]{../examples/JuliaCon/fig3/exponential.pdf}\hfill
  \includegraphics[width=.25\textwidth]{../examples/JuliaCon/fig3/chisquared.pdf}\hfill
  \includegraphics[width=.25\textwidth]{../examples/JuliaCon/fig3/cauchy.pdf}
  

  \caption{Various parametrically constructed p-boxes with interval parameters}
  \label{fig:figure3}
  
\end{figure*}


The vectors $u$ and $d$ are a collection of "real" or "physical" evaluations of a the random variable $X$, as opposed to cdf evaluations. Note that although the probability levels $p$ are uniformly spaced, $u$ and $d$ are generally not. The vectors $u$ and $d$ can then serve as bounds on the monotonic functions $\overline{F}^{-1}$ and $\underline{F}^{-1}$:

\begin{align*}
  u[i] &\leq \overline{F}^{-1}(p) & p_{i} \in [p_{i}, p_{i+1} )\\ 
  d[i] &\geq \underline{F}^{-1}(p) & p_{i} \in [p_{i}, p_{i+1})
\end{align*}

That is, in the region $[p_{i}, p_{i+1})$, $u[i]$ is a lower bound on $\overline{F}^{-1}$ and $d[i]$ is an upper bound on $\underline{F}^{-1}$. Note that the definition of the regions $[p_{i}, p_{i+1})$ or $(p_{i}, p_{i+1}]$ doesn't matter, as long as the choice is consistent. In the case that the p-box is a precise distribution and $\overline{F}^{-1}(p) = \underline{F}^{-1}(p)$ gives:  $u[i+1] = d[i]$. Figure \ref{fig:figure2} shows examples of a distribution and a p-box with various outer representations.

A minimal p-box structure is as follows:

\begin{lstlisting}[language = Julia]
  using IntervalArithmetic

  struct pbox{T <: Real}
    u :: Vector{T}    # left discrete inverse
    d :: Vector{T}    # right discrete inverse
    m :: Interval{T}  # interval mean
    v :: Interval{T}  # interval variance
    shape :: String   # shape
  end
\end{lstlisting}

\noindent where the vectors $u$ and $d$ are the "up" and "down", or the "left" and "right", edges of the p-box\footnote{There can be confusion since $\overline{F}$ is the upper bound on the cdf, but $\overline{F}^{-1}$ is the lower bound on the random variable, i.e. the left edge of the p-box. We therefore $\overline{F}$ the "up" or "left" edge, and $\underline{F}$ the "down" or the "right" edge.}. The intervals $m$ and $v$ are the interval moments, and the $shape$ is a string containing any distribution family information. An example of a normal distribution constructor is as follows:


\begin{lstlisting}[language = Julia]
  using Distributions
  const steps = 200

  function normal(m :: Interval, std :: Interval)

    ps = range(0, 1, length = steps+1)

    ps_u = ps[1:end-1]
    ps_d = ps[2:end]

    u1 = quantile.(Normal(m.lo, std.lo), ps_u)
    d1 = quantile.(Normal(m.hi, std.lo), ps_d)

    u2 = quantile.(Normal(m.lo, std.hi), ps_u)
    d2 = quantile.(Normal(m.hi, std.hi), ps_d)

    u = min(u1, u2)
    d = max(d1, d2)

    return pbox(u, d, m, std^2, "normal")
  end

\end{lstlisting}

Other distributional p-boxes are constructed in a similar way. A evaluation of the cdf in terms of this representation is as follows: 

\begin{lstlisting}[language = Julia]
  function cdf(F :: pbox, x :: Real)
    
    u = F.u; d = F.d; n = length(u);
    
    cdf_u = 1 - sum(x .< u)/n;
    cdf_d = 1 - sum(x .<= d)/n;
    
    return interval(cdf_d, cdf_u)
  end

\end{lstlisting}

A horizontal cut (an evaluation of the inverse cdf) and a sample of a p-box can be found as follows: 

\begin{lstlisting}[language = Julia]
  function cut(F :: pbox, p :: Real)

    u = F.u; d = F.d; n = length(u);
    ps = range(0, 1, length = n+1);

    ind_u = findlast(ps[1:end-1] .<= p)
    ind_d = findfirst(ps[2:end] .>= p)
    
    return interval(u[ind_u], d[ind_d])
  end

  # a random sample is a random cut
  rand(F :: pbox, N :: Int) = cut.(F, rand(N))

\end{lstlisting}

And finally the probability measure, which we call the \textit{mass} for short:

\begin{lstlisting}[language = Julia]
  function mass(F :: pbox, x :: Interval)

    cdfHi = cdf(F, x.hi)  
    cdfLo = cdf(F, x.lo)  # returns intervals

    ub = cdfHi.hi - cdfLo.lo
    lb = max(0, cdfHi.lo - cdfLo.hi)

    return interval(lb, ub)
  end

\end{lstlisting}

Other available functions on p-boxes which we do not describe here are:

\begin{itemize}
  \item \textbf{env} or $\cup$: takes the envelope or hull of two p-boxes. A convex set-union of the probability sets
  \item \textbf{imp} or $\cap$: imposition or intersection of two p-boxes. Set-intersection of probability sets
  \item \textbf{subseteq} or $\subseteq$: checks if a p-box is a subset or equal to another
  \item \textbf{in} or $\in$: checks if a distribution is a member of a p-box
  \item \textbf{mixture}: a stochastic mixture of p-boxes.
\end{itemize}

\subsection{Where do p-boxes come from?}

In this section we discuss some situations where p-boxes arise naturally.


\subsubsection{Partial distributional information} \hfill \break

In situations where the parametric family of your uncertainty is known, for example it is known that $X$ is normally distributed $X \sim N(\mu_{X}, \sigma_{X}^{2})$ or that $Y$ is a uniform $Y \sim U(a_{Y},b_{Y})$, but the precise parameters to these functions are unknown, then p-boxes are a natural representation for such uncertainty. In the case that bounds are known on parameters, then bounds on the cdf can be found using the interval parameters and the inverse cdf of the parametric family. Figure \ref{fig:figure3} shows a variety of p-boxes constructed this way.


\subsubsection{Partial moment information} %\hfill \break
\label{section:Moments}
%It is well known that a random variables distribution function cannot be 


\subsubsection{Operations involving intervals and distributions} \hfill \break

An arithmetic operation involving an interval and a precise distribution will produce set of distributions, i.e. a unique distribution for every real number in the interval, whose p-box bounds can be found. The following function performs a binary operation between a p-box and an interval:

\begin{lstlisting}[language = Julia]
  using MomentArithmetic

  function binary(F :: pbox, X :: Interval, op)

    u_z = op.(Interval.(F.u), X)
    d_z = op.(Interval.(F.d), X)

    u_z_ = getfield.(u_z, :lo)  # get right edges
    d_z_ = getfield.(d_z, :hi)  # get left edges

    # for moments
    F_range = interval(F.u[1], F.d[end])
    F_moms = Moments(F.m, F.v, F_range);
    Z_moms = op(F_moms, X);

    return pbox(u_z_, d_z_, Z_moms.mean, 
    Z_moms.var, F.shape)
  end
\end{lstlisting}

How the moment transformations are performed are not described here, however are available in \textit{MomentArithmetic.jl} \cite{ferson2021distribution}. Figure \ref{fig:figure4} shows an example of an operation between a precise distribution and an interval.


\begin{figure}[htp]

  \centering
  \includegraphics[width=0.12\textwidth]{../examples/JuliaCon/fig4/fig4_dist.pdf}
  \raisebox{8.0mm}{\noindent\Large*}
  \includegraphics[width=0.12\textwidth]{../examples/JuliaCon/fig4/fig4_in.pdf}
  \raisebox{9.0mm}{{\Large$\rightarrow$}}
  \includegraphics[width=0.12\textwidth]{../examples/JuliaCon/fig4/fig4_pbox.pdf}
  

  \caption{Illustration of a p-box resulting from a binary operation between a distribution and an interval}
  \label{fig:figure4}
  
\end{figure}


\subsubsection{When dependencies between distributions are unknown} \hfill \break

In the case where a binary operation is performed between two distributions but whose dependence or correlation is unknown, then a single output distribution also cannot be defined. However a bounding p-box can be computed. This is the original question of Kolmogorov as described in the introduction. We call such operations between p-boxes where the marginals are fixed but no joint information is known \textit{Fréchet} operations, after the mathematician Maurice R. Fréchet, who first discovered how to bound joint distributions with fixed marginals \cite{frechet1935generalisation}. How operations between p-boxes with partial or no dependence information is known is described in section \ref{sec:pboxBinary}. Figure \ref{fig:figure5} illustrates a Fréchet operation between a uniform and a normal distribution.

\begin{figure}[htp]

  \centering
  \includegraphics[width=0.12\textwidth]{../examples/JuliaCon/fig5/fig5_dist1.pdf}
  \raisebox{8.0mm}{\noindent\Large+\small$_{\text{Fr{\'e}chet}}$}
  \includegraphics[width=0.12\textwidth]{../examples/JuliaCon/fig5/fig5_dist2.pdf}
  \raisebox{9.0mm}{{\Large$\rightarrow$}}
  \includegraphics[width=0.12\textwidth]{../examples/JuliaCon/fig5/fig5_pbox.pdf}
  

  \caption{Illustration of a p-box resulting from a Fréchet operation}
  \label{fig:figure5}
  
\end{figure}


\subsubsection{Outer approximations of precise distributions} \hfill \break

The outer approximation of precise distributions described in section \ref{sec:outer_approx} also leads naturally to a p-box. All probabilistic quantities of discretised distribution functions will be intervals, which shrink to precise quantities as the discretisation increases.

\subsubsection{Inferential methods when data is limited or bad} \hfill \break

Probability distributions are almost always constructed from data or measurements, or otherwise are assumed as a model or derived from expert opinion. Usually to construct a probability distribution accurately, a large amount of high quality data is required. P-boxes arise naturally when this is not the case, for example if the data are intervals instead of points, or when data is sparse or not reliable and, an exact distribution function is difficult to prescribe.

An example is robust bayesian analysis, where the analyst does not have enough information to reliably establish a single prior and likelihood function, but instead has a collection of priors and likelihoods, leading to a set of posteriors distributions, to which a p-box can be identified.

A method which is growing in popularity are inferential models \cite{martin2015inferential}. These generalise classical inference methods to account for limited statistical knowledge, and gives a statistical inference theory which is non-additive (as opposed to classical probability which is additive). These inference procedures, often based on random sets, give certain validity and performance guarantees. Inferential models can viewed from the perspective of bounding sets of distributions, and is closely linked to possibility theory \cite{liu2020inferential}. A p-box can be prescribed to such structures, leading to structures like confidence boxes or c-boxes \cite{fersoncomputing}, which follow the same arithmetic rules as p-boxes. 

\subsection{Relationship to other ideas}

\subsubsection{Random set theory}

\subsubsection{Possibility theory}


%\section{Operations on p-boxes}

%\section{P boxes}

\subsection{Bivariate p-boxes}


\section{P-box arithmetic}
\label{sec:pboxarithmetic}

\subsection{Uniary operations}

\subsection{Binary operations}
\label{sec:pboxBinary}

\section{An uncertain programming language}
\label{sec:additional_faci}

The long term goal of such a framework is to create a fully uncertain programming language, where any computer variable may be represented as an interval, distribution, p-box or other uncertain quantity. Such a framework would allow for uncertain extensions of deterministic functions to be computed in an automatic, rigorous and tight fashion. In this section we argue why Julia is an ideal target language for such a framework, and discuss some of the remaining theoretical tasks required to make such a goal a reality. 

\section*{Acknowledgements}

The authors would like to thank the support of this work from the Engineering Physical Sciences Research Council (EPSRC) iCase studentship award. This research is funded by the EPSRC with grant number EP/R006768/1, which is acknowledged for its funding and support. This work has been carried out within the frame- work of the EUROfusion Consortium and has received funding from the Euratom research and training programme 2014–2018 and 2019–2020 under grant agreement number 633053. The views and opinions expressed herein do not necessarily reflect those of the European Commission.

\input{bib.tex}

\end{document}

% Inspired by the International Journal of Computer Applications template
